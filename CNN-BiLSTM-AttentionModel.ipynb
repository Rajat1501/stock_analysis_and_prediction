{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc540818",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Bidirectional, LSTM,\n",
    "                                     Dropout, Dense, Add, GlobalAveragePooling1D, Attention)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737383cd",
   "metadata": {},
   "source": [
    "DATA PREPRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_scale_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads, pre-processes, and scales data with the train-val-test split.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    features_to_use = [\n",
    "        'open', 'high', 'low', 'close', 'volume', 'sma5', 'sma20', 'ema5', 'ema20',\n",
    "        'upperband', 'middleband', 'lowerband', 'macd1226', 'MOM10', 'RSI14',\n",
    "        'slowk', 'slowd', 'WILLR', 'ATR'\n",
    "    ]\n",
    "    df = df[features_to_use].astype('float32')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df['TomorrowClose'] = df['close'].shift(-1)\n",
    "    df['Target'] = (df['TomorrowClose'] > df['close']).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    features = df.drop(['TomorrowClose', 'Target'], axis=1)\n",
    "    target = df['Target']\n",
    "\n",
    "    # Splitting the data\n",
    "    train_size = int(len(features) * 0.8) \n",
    "    val_size = int(len(features) * 0.1) # 10% for validation\n",
    "\n",
    "    train_features = features[:train_size]\n",
    "    val_features = features[train_size : train_size + val_size]\n",
    "    test_features = features[train_size + val_size :]\n",
    "\n",
    "    train_target = target[:train_size]\n",
    "    val_target = target[train_size : train_size + val_size]\n",
    "    test_target = target[train_size + val_size :]\n",
    "\n",
    "    # Scaling after splitting\n",
    "    scaler = MinMaxScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    val_features_scaled = scaler.transform(val_features)\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "    return (train_features_scaled, train_target.values,\n",
    "            val_features_scaled, val_target.values,\n",
    "            test_features_scaled, test_target.values,\n",
    "            scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa3e99",
   "metadata": {},
   "source": [
    "DATA PIPELINE; SUITABLE INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(features, targets, sequence_length, batch_size):\n",
    "    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=features,\n",
    "        targets=targets,\n",
    "        sequence_length=sequence_length,\n",
    "        sequence_stride=1,\n",
    "        sampling_rate=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf1c16",
   "metadata": {},
   "source": [
    "BUILDING THE CNN-BiLSTM-Attention MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficient_model(input_shape, dropout=0.3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=7, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    lstm_out = Bidirectional(LSTM(units=128, return_sequences=True))(x)\n",
    "    attention_out = Attention()([lstm_out, lstm_out])\n",
    "    x = GlobalAveragePooling1D()(attention_out)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba115a",
   "metadata": {},
   "source": [
    "EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1df8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    FILEPATH = '/Users/rajataggarwal/Downloads/NIFTY100_5mindata/DMART_with_indicators_.csv'\n",
    "    SEQUENCE_LENGTH = 75 * 5\n",
    "    BATCH_SIZE = 64\n",
    "    MODEL_SAVE_PATH = 'best_stock_predictor.keras'\n",
    "    SCALER_SAVE_PATH = 'data_scaler.joblib'\n",
    "\n",
    "    # Load and process data\n",
    "    print(\"Loading, scaling, and splitting data...\")\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y, scaler = prepare_and_scale_data(FILEPATH)\n",
    "\n",
    "    # Create high-performance tf.data pipelines\n",
    "    train_dataset = create_tf_dataset(train_X, train_y, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "    val_dataset = create_tf_dataset(val_X, val_y, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "    test_dataset = create_tf_dataset(test_X, test_y, SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "    print(\"TF.data pipelines created.\")\n",
    "\n",
    "    # Build the model\n",
    "    input_shape = (SEQUENCE_LENGTH, train_X.shape[1])\n",
    "    model = build_efficient_model(input_shape)\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) #Binary crossentropy loss function since model itself is about classifying trends.\n",
    "    model.summary()\n",
    "\n",
    "    # --- ADDED: ModelCheckpoint to save the best model ---\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "        ModelCheckpoint(filepath=MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- ADDED: Save the scaler for real-world prediction ---\n",
    "    print(f\"Saving the data scaler to '{SCALER_SAVE_PATH}'...\")\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "    print(\"Scaler saved.\")\n",
    "\n",
    "    # --- ADDED: Comprehensive Evaluation ---\n",
    "    print(\"\\nEvaluating model on the unseen test set...\")\n",
    "    \n",
    "    # Load the best model saved by ModelCheckpoint for final evaluation\n",
    "    best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "    y_pred_proba = best_model.predict(test_dataset)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    y_test_aligned = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "    y_pred = y_pred[:len(y_test_aligned)]\n",
    "\n",
    "    # Calculate and print all relevant metrics\n",
    "    accuracy = accuracy_score(y_test_aligned, y_pred)\n",
    "    precision = precision_score(y_test_aligned, y_pred)\n",
    "    recall = recall_score(y_test_aligned, y_pred)\n",
    "    f1 = f1_score(y_test_aligned, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test_aligned, y_pred_proba[:len(y_test_aligned)])\n",
    "\n",
    "    print(\"\\n--- Final Model Performance ---\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\") \n",
    "    #What it means: Overall, what percentage of predictions were correct?\n",
    "    \n",
    "    print(f\"\\nTest Precision: {precision:.4f}\") \n",
    "    #What it means: Of all the times the model predicted 'UP', how often was it right? (Measures signal quality)\n",
    "\n",
    "    print(f\"\\nTest Recall: {recall:.4f}\")\n",
    "    #What it means: Of all the times the market actually went 'UP', how many did the model catch? (Measures opportunity capture)\n",
    "\n",
    "    print(f\"\\nTest F1-Score: {f1:.4f}\")\n",
    "    #What it means: A balanced score between Precision and Recall. \n",
    "\n",
    "    print(f\"\\nTest ROC AUC: {roc_auc:.4f}\")\n",
    "    # What it means: Measures the model's ability to distinguish between the 'UP' and 'DOWN' classes across all probability thresholds.\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
